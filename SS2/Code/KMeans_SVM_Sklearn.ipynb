{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import numpy as np \n",
    "\n",
    "def load_data(path = \"/Users/charles/MLGT/SS2/Data/\"):\n",
    "    # Get data from each line with label, doc_id, index & tfidf of its vocab\n",
    "    def sparse_to_dense(sparse_r_d, vocab_size):\n",
    "        # Init list size vocal size to store vocal \n",
    "        r_d = [0.0 for _ in range(vocab_size)]\n",
    "            \n",
    "        # Split space & : in context data of each line \n",
    "        # Get index (id vocal of each line) & tfidfs\n",
    "        indices_and_tfidfs = sparse_r_d.split()\n",
    "        for index_and_tfidf in indices_and_tfidfs:\n",
    "            index = int(index_and_tfidf.split(':')[0])\n",
    "            tfidf = float(index_and_tfidf.split(':')[1])\n",
    "            r_d[index] = tfidf\n",
    "        return np.array(r_d)    \n",
    "                \n",
    "    # Open file (newsgroup, id, context)\n",
    "    with open(path + \"data_tf_idf.txt\") as f:\n",
    "        data_lines = f.read().splitlines()\n",
    "    # Get size file vocal TF-IDF\n",
    "    with open(path + \"words_idfs.txt\") as f:\n",
    "        vocab_size = len(f.read().splitlines())\n",
    "\n",
    "    # Member store info of data points: tf_idf, news group, file name of text d\n",
    "    data, labels = [], []\n",
    "    # Iterating sequence of pairs with counter\n",
    "    for data_id, d in enumerate(data_lines):\n",
    "        features = d.split('<fff>')\n",
    "        label, doc_id = int(features[0]), int(features[1])\n",
    "        r_d = sparse_to_dense(sparse_r_d=features[2], vocab_size=vocab_size)\n",
    "\n",
    "        # Append data & labels\n",
    "        data.append(r_d)\n",
    "        labels.append(label)\n",
    "    return data, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, label = load_data()\n",
    "\n",
    "# Split Data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_text, y_text, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use csr_matrix to create a sparse matrix with efficient row slicing\n",
    "# Improve time to computing\n",
    "from scipy.sparse import csr_matrix\n",
    "data = csr_matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "def compute_accuracy(predicted_y, expected_y):\n",
    "    # Check boolean True = 1, False = 0 \n",
    "    matches = np.equal(predicted_y, expected_y)\n",
    "    accuracy = np.sum(matches.astype(float)) / len(expected_y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "618af10fdd433c84be79e5d9cef7a85d74ad68be7e2e9dd9461a47b527f16862"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
