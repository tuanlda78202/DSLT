{"cells":[{"cell_type":"markdown","metadata":{"id":"a2GRDX8G92r3"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2051,"status":"ok","timestamp":1621823801135,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"},"user_tz":-420},"id":"fMsUmWmLIuS0","outputId":"e98f5024-e9f1-4562-a0c9-d9adbf44bf29"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1621823801137,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"},"user_tz":-420},"id":"5vKU4ygLIzHx"},"outputs":[],"source":["MAX_SENTENCE_LENGTH = 500\n","NUM_CLASSES = 20"]},{"cell_type":"markdown","metadata":{"id":"b4-aM50W-DSH"},"source":["## RNN"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1621823801139,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"},"user_tz":-420},"id":"SvO5jVQG-AIX"},"outputs":[],"source":["class RNN:\n","  def __init__(self,\n","               vocab_size,\n","               embedding_size,\n","               lstm_size,\n","               batch_size):\n","    self._vocab_size = vocab_size\n","    self._embedding_size = embedding_size\n","    self._lstm_size = lstm_size\n","    self._batch_size = batch_size\n","\n","    self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_SENTENCE_LENGTH])\n","    self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n","    self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n","    self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n","\n","\n","  def embedding_layer(self, indices):\n","    pretrained_vectors = []\n","    pretrained_vectors.append(np.zeros(self._embedding_size))\n","    np.random.seed(2021)\n","    for _ in range (self._vocab_size + 1):\n","      pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n","\n","    pretrained_vectors = np.array(pretrained_vectors)\n","\n","    self._embedding_matrix = tf.get_variable(\n","        name='embedding',\n","        shape=(self._vocab_size + 2, self._embedding_size),\n","        initializer=tf.constant_initializer(pretrained_vectors)\n","    )\n","    return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n","\n","\n","  def LSTM_layer(self, embeddings):\n","    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n","    zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n","    initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n","\n","    lstm_inputs = tf.unstack(\n","        tf.transpose(embeddings, perm=[1, 0, 2])\n","    )\n","    lstm_outputs, last_state = tf.nn.static_rnn(\n","        cell=lstm_cell,\n","        inputs=lstm_inputs,\n","        initial_state=initial_state,\n","        sequence_length=self._sentence_lengths\n","    ) # a length-500 list of [num_docs, lstm_size]\n","    lstm_outputs = tf.unstack(\n","        tf.transpose(lstm_outputs, perm=[1, 0, 2])\n","    )\n","    lstm_outputs = tf.concat(\n","        lstm_outputs,\n","        axis=0\n","    ) # [num_docs * MAX_SENTENCE_LENGTH, lstm_size]\n","\n","    # self._mask : [num_docs * MAX_SENT_LENGTH, ]\n","    mask = tf.sequence_mask(\n","        lengths=self._sentence_lengths,\n","        maxlen=MAX_SENTENCE_LENGTH,\n","        dtype=tf.float32\n","    ) # [num_docs, MAX_SENTENCE_LENGTH]\n","    mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n","    mask = tf.expand_dims(mask, -1)\n","\n","    lstm_outputs = mask * lstm_outputs\n","    lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n","    lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n","    lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n","        tf.cast(self._sentence_lengths, tf.float32),\n","        -1\n","    ) # expand_dims only works with tensor of float type\n","      # [num_docs, lstm_size]\n","\n","    return lstm_outputs_average\n","\n","\n","  def build_graph(self):\n","    embeddings = self.embedding_layer(self._data)\n","    lstm_outputs = self.LSTM_layer(embeddings)\n","\n","    weigths = tf.get_variable(\n","        name = 'final_layer_weights',\n","        shape = (self._lstm_size, NUM_CLASSES),\n","        initializer = tf.random_normal_initializer(seed = 2021)\n","    )\n","    biases = tf.get_variable(\n","        name = 'final_layer_biases',\n","        shape = (NUM_CLASSES),\n","        initializer = tf.random_normal_initializer(seed = 2021)\n","    )\n","    logits = tf.matmul(lstm_outputs, weigths) + biases\n","\n","    labels_one_hot = tf.one_hot(\n","        indices = self._labels,\n","        depth = NUM_CLASSES,\n","        dtype = tf.float32\n","    )\n","\n","    loss = tf.nn.softmax_cross_entropy_with_logits(\n","        labels = labels_one_hot,\n","        logits = logits\n","    )\n","    loss = tf.reduce_mean(loss)\n","\n","    probs = tf.nn.softmax(logits)\n","    predicted_labels = tf.argmax(probs, axis = 1)\n","    predicted_labels = tf.squeeze(predicted_labels)\n","    return predicted_labels, loss\n","\n","\n","  def trainer(self, loss, learning_rate):\n","    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n","    return train_op"]},{"cell_type":"markdown","metadata":{"id":"kDdvKlTnPb2q"},"source":["## Data Reader "]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1621823801140,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"},"user_tz":-420},"id":"DIFlPwJrLpmr"},"outputs":[],"source":["class DataReader:\n","  def __init__(self, data_path, batch_size):\n","    self._batch_size = batch_size\n","    with open(data_path) as f:\n","      d_lines = f.read().splitlines()\n","\n","    self._data = []\n","    self._labels = []\n","    self._sentence_lengths = []\n","    self._final_tokens = [] \n","    for data_id, line in enumerate(d_lines):\n","      features = line.split('<fff>')\n","      label, doc_id, sentence_length = int(features[0]), int(features[1]), int(features[2])\n","      tokens = features[3].split()\n","\n","      self._data.append(tokens)\n","      self._sentence_lengths.append(sentence_length)\n","      self._labels.append(label)\n","      self._final_tokens.append(tokens[-1])\n","\n","    self._data = np.array(self._data)\n","    self._labels = np.array(self._labels)\n","    self._sentence_lengths = np.array(self._sentence_lengths)\n","    self._final_tokens = np.array(self._final_tokens)\n","\n","    self._num_epoch = 0\n","    self._batch_id = 0\n","    self._size = len(self._data)\n","\n","  def next_batch(self):\n","    start = self._batch_id * self._batch_size\n","    end = start + self._batch_size\n","    self._batch_id += 1\n","\n","    if end + self._batch_size > len(self._data):\n","      self._size = end\n","      end = len(self._data)\n","      start = end - self._batch_size\n","      self._num_epoch += 1\n","      self._batch_id = 0\n","      indices = list(range(len(self._data)))\n","      random.seed(2021)\n","      random.shuffle(indices)\n","      self._data, self._labels, self._sentence_lengths, self._final_tokens = self._data[indices], self._labels[indices], self._sentence_lengths[indices], self._final_tokens[indices]\n","\n","    return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end], self._final_tokens[start:end]"]},{"cell_type":"markdown","metadata":{"id":"Hw_1g0foLNYq"},"source":["## Training "]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1621823801141,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"},"user_tz":-420},"id":"MN3uuRdcPJVa"},"outputs":[],"source":["loss_report = []\n","accuracy_report = []"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1621823801142,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"},"user_tz":-420},"id":"dmGzKMSILM9r"},"outputs":[],"source":["def train_and_evaluate_RNN():\n","  with open('/Users/charles/MLGT/ss4/data/vocab_raw.txt') as f:\n","      vocab_size = len(f.read().splitlines())\n","\n","  tf.set_random_seed(2021)\n","  rnn = RNN(\n","      vocab_size=vocab_size,\n","      embedding_size=300,\n","      lstm_size=50,\n","      batch_size=50\n","  )\n","  predicted_labels, loss = rnn.build_graph()\n","  train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n","\n","  with tf.Session() as sess:\n","    train_data_reader = DataReader(\n","        data_path='/Users/charles/MLGT/ss4/data/20news_train_encoded.txt',\n","        batch_size=50,\n","    )\n","    test_data_reader = DataReader(\n","        data_path='/Users/charles/MLGT/ss4/data/20news_test_encoded.txt',\n","        batch_size=50,\n","    )\n","    step = 0\n","    MAX_STEP = 10000\n","    \n","    sess.run(tf.global_variables_initializer())\n","    while step < MAX_STEP:\n","        next_train_batch = train_data_reader.next_batch()\n","        train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n","        plabels_eval, loss_eval, _ = sess.run(\n","            [predicted_labels, loss, train_op],\n","            feed_dict={\n","                rnn._data: train_data,\n","                rnn._labels: train_labels,\n","                rnn._sentence_lengths: train_sentence_lengths,\n","                rnn._final_tokens: train_final_tokens\n","            }\n","        )\n","        step += 1\n","        if step % 20 == 0:\n","          loss_report.append(loss_eval)\n","          print('loss: {}'.format(loss_eval))\n","        if train_data_reader._batch_id == 0:\n","          num_true_preds = 0\n","          while True:\n","            next_test_batch = test_data_reader.next_batch()\n","            test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n","            test_plabels_eval = sess.run(\n","                predicted_labels,\n","                feed_dict={\n","                    rnn._data: test_data,\n","                    rnn._labels: test_labels,\n","                    rnn._sentence_lengths: test_sentence_lengths,\n","                    rnn._final_tokens: test_final_tokens\n","                }\n","            )\n","            matches = np.equal(test_plabels_eval, test_labels)\n","            num_true_preds += np.sum(matches.astype(float))\n","\n","            if test_data_reader._batch_id == 0:\n","              break\n","\n","          accuracy_report.append(num_true_preds * 100. / test_data_reader._size)        \n","          print('Epoch: {}'.format(train_data_reader._num_epoch))\n","          print('Accuracy on test data: {}'.format(num_true_preds * 100. / test_data_reader._size))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4078125,"status":"ok","timestamp":1621834717699,"user":{"displayName":"Vo Thuc Khanh Huyen FX06758","photoUrl":"","userId":"10987621998736532516"},"user_tz":-420},"id":"mIG0ySbRQBrC","outputId":"f144b409-1cf5-4db7-ab9d-6b904b62602b"},"outputs":[],"source":["train_and_evaluate_RNN()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN7ZWbTtlNNx2bXf5RK88gS","collapsed_sections":[],"mount_file_id":"1J_R4SRohC1PtV8uAnfSE_dja57AUC_9j","name":"Session4-RNN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
