{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing text data with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF with a context d in D (corpus):\n",
    "\n",
    "$r_d = [tf-idf(w_1, d, D), tf-idf(w_2, d, D), ..., tf-idf(w_{|V|}, d, D)]$\n",
    "\n",
    "with, $r_d \\in R^{|V|}$ is a vector $|V|$ dims and $V = {w_i}$ is a dictionary (all words appear in $D$) respect to $D$\n",
    "\n",
    "- Inside:\n",
    "\n",
    "$tf-idf(w_i, d, D) = tf(w_i, d) * idf(w_i, D)$\n",
    "\n",
    "with,\n",
    "\n",
    "$tf(w_i, d) = \\dfrac{f(w_i, d)}{max(f(w_j, d): w_j \\in V)}$\n",
    "\n",
    "$idf(w_i, D) = log_{10}^{\\dfrac{|D|}{|d' \\in D: w_i \\in d'|}}$\n",
    "\n",
    "- Identify dictionary V:\n",
    "\n",
    "  - With each context $d$ in $D$:\n",
    "    - Separate d to some word by punctuation, then collect $W_d$\n",
    "    - Delete stop words from $W_d$\n",
    "    - Convert word to original (stemming), then collect $W_d$\n",
    "  - Finally:\n",
    "    $V = $ Intersection of $W_d$ with $d \\in D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. TF-IDF Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Path\n",
    "import os\n",
    "# Module Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "# Other lib\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science', 'is', 'the', 'sexiest', 'job', 'of', 'the', '21st', 'century,', 'data', 'scientist', 'is', 'my', 'dream']\n",
      "['machine', 'learning', 'is', 'the', 'key', 'for', 'data', 'science,', 'machine', 'learning', 'is', 'my', 'life']\n",
      "{'data', 'learning', 'for', 'the', 'job', 'my', 'science,', 'scientist', 'of', 'science', 'dream', '21st', 'is', 'sexiest', 'life', 'century,', 'key', 'machine'}\n"
     ]
    }
   ],
   "source": [
    "# Init data\n",
    "sentence_1 = \"Data Science is the sexiest job of the 21st century, Data Scientist is my dream\"\n",
    "sentence_2 = \"Machine Learning is the key for Data Science, Machine Learning is my life\"\n",
    "\n",
    "# Process data \n",
    "sentence_1, sentence_2 = sentence_1.lower().split(), sentence_2.lower().split()\n",
    "sentence_1n2 = set(sentence_1).union(sentence_2)\n",
    "\n",
    "print(sentence_1, sentence_2, sentence_1n2, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science', 'sexiest', 'job', '21st', 'century,', 'data', 'scientist', 'dream']\n",
      "['machine', 'learning', 'key', 'data', 'science,', 'machine', 'learning', 'life']\n",
      "['data', 'learning', 'job', 'science,', 'scientist', 'science', 'dream', '21st', 'sexiest', 'life', 'century,', 'key', 'machine']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charles/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download file stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')         \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter sentence by stopwords with module ntlk\n",
    "ft_sentence_1 = [word for word in sentence_1 if word not in stop_words]\n",
    "ft_sentence_2 = [word for word in sentence_2 if word not in stop_words]\n",
    "ft_sentence_1n2 = [word for word in sentence_1n2 if word not in stop_words]\n",
    "\n",
    "print(ft_sentence_1, ft_sentence_2, ft_sentence_1n2, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>learning</th>\n",
       "      <th>job</th>\n",
       "      <th>science,</th>\n",
       "      <th>scientist</th>\n",
       "      <th>science</th>\n",
       "      <th>dream</th>\n",
       "      <th>21st</th>\n",
       "      <th>sexiest</th>\n",
       "      <th>life</th>\n",
       "      <th>century,</th>\n",
       "      <th>key</th>\n",
       "      <th>machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  learning  job  science,  scientist  science  dream  21st  sexiest  \\\n",
       "0     2         0    1         0          1        1      1     1        1   \n",
       "1     1         2    0         1          0        0      0     0        0   \n",
       "\n",
       "   life  century,  key  machine  \n",
       "0     0         1    0        0  \n",
       "1     1         0    1        2  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict.fromkeys: create  dictionary with keys from iterable and values set to value.\n",
    "dictA, dictB = dict.fromkeys(ft_sentence_1n2, 0), dict.fromkeys(ft_sentence_1n2, 0)\n",
    "# Check element for each sentence \n",
    "for _ in ft_sentence_1:\n",
    "    dictA[_] = dictA.get(_, 0) + 1 \n",
    "for _ in ft_sentence_2:\n",
    "    dictB[_] = dictB.get(_, 0) + 1 \n",
    "\n",
    "# Create DF\n",
    "df = pd.DataFrame([dictA, dictB])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 0.8129133566428556,\n",
       " 'job': 0.5569716761534184,\n",
       " 'scientist': 0.5569716761534184,\n",
       " 'science': 0.5569716761534184,\n",
       " 'dream': 0.5569716761534184,\n",
       " '21st': 0.5569716761534184,\n",
       " 'sexiest': 0.5569716761534184,\n",
       " 'century,': 0.5569716761534184}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute TF-IDF\n",
    "def compute_tfidf(word_dict):\n",
    "    # Compute TF \n",
    "    def compute_tf(word_dict):\n",
    "        tf_dict = {}\n",
    "        for key, val in word_dict.items():\n",
    "            tf_dict[key] = word_dict[key] / max(word_dict.values())\n",
    "        return tf_dict\n",
    "    \n",
    "    # Compute IDF \n",
    "    def compute_idf(word_dict):\n",
    "        # |D| is number of elements D (chose dict)\n",
    "        N = len(word_dict)\n",
    "        idf_dict = {}\n",
    "        for key, val in word_dict.items():\n",
    "            if val != 0:\n",
    "                idf_dict[key] = math.log10(N / val)\n",
    "        return idf_dict   \n",
    "    \n",
    "    # Compute TF-IDF\n",
    "    tf_dict, idf_dict, tfidf_dict = compute_tf(word_dict), compute_idf(word_dict), {}\n",
    "    for key_tf, val_tf in tf_dict.items():\n",
    "        for key_idf, val_idf in idf_dict.items():\n",
    "            if key_idf == key_tf:\n",
    "                tfidf_dict[key_tf] = val_tf * val_idf\n",
    "    return tfidf_dict\n",
    "\n",
    "# Convert DF with TF-IDF\n",
    "compute_tfidf(dictA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read & Gather Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Path\n",
    "import os\n",
    "# Module Stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/charles/MLGT/SESSION 1/Data/stop_word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3b/0j1fpv595z7clkp9kz6d2t880000gn/T/ipykernel_20728/2503415888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/charles/MLGT/SESSION 1/Data/20news-bydate/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mgather_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/3b/0j1fpv595z7clkp9kz6d2t880000gn/T/ipykernel_20728/2503415888.py\u001b[0m in \u001b[0;36mgather_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Create Dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Read stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/charles/MLGT/SESSION 1/Data/stop_word\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Stemming data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/charles/MLGT/SESSION 1/Data/stop_word'"
     ]
    }
   ],
   "source": [
    "def gather_data(path):\n",
    "    # Get list dir of folder & news_group\n",
    "    # listdir(): get the list of all files and directories in the specified directory. \n",
    "    dirs = [path + dir_name + \"/\"\n",
    "            for dir_name in os.listdir(path)\n",
    "                if not os.path.isfile(path + dir_name)]\n",
    "    # Assign folder train & test dir\n",
    "    train_dir, test_dir = (dirs[0], dirs[1]) if \"train\" in dirs else (dirs[1], dirs[0])\n",
    "    # Crawl news group\n",
    "    list_newsgroup = [news for news in os.listdir(train_dir)]\n",
    "    list_newsgroup.sort()\n",
    "    \n",
    "    # Create Dictionary\n",
    "    # Read stop words\n",
    "    with open(\"/Users/charles/MLGT/SESSION 1/Data/stop_word\") as f:\n",
    "        stop_words = f.read().splitlines()\n",
    "    # Stemming data \n",
    "    \n",
    "\n",
    "path = \"/Users/charles/MLGT/SESSION 1/Data/20news-bydate/\" \n",
    "gather_data(path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "618af10fdd433c84be79e5d9cef7a85d74ad68be7e2e9dd9461a47b527f16862"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
